{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Guassian_Mixture_Model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"DqtxGtJPCemW","colab_type":"code","colab":{}},"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fMghAZuKEwPK","colab_type":"code","colab":{}},"source":["auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","# https://drive.google.com/open?id=1ZmDAn2whHFm4ok6jFc2gpH4PnyDFLvGN"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KIYFpRH9Ezv8","colab_type":"code","colab":{}},"source":["#downloaded = drive.CreateFile({'id':\"1ZmDAn2whHFm4ok6jFc2gpH4PnyDFLvGN\"})   # replace the id with id of file you want to access\n","#downloaded.GetContentFile('faithful.txt')        # replace the file name with your file"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UGDnj4REWAg3","colab_type":"code","colab":{}},"source":["downloaded = drive.CreateFile({'id':\"13xmiSh8AgQjj6BJ7Dxu_byo59bnlUT8h\"})   # replace the id with id of file you want to access\n","downloaded.GetContentFile('data.csv')        # replace the file name with your file"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XZDDMsLGFQBo","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","\n","class GMM:\n","    \"\"\"\n","    Implements the expectation-maximisation (EM) algorithm for the\n","    Gaussian mixture model (GMM). The algorithm is based on the\n","    pseudo-code described in the book by C. Bishop \"Pattern Recognition\n","    and Machine Learning\", chapter 9.\n","    \"\"\"\n","    def __init__(self,\n","                 n_components,\n","                 means=None,\n","                 covariances=None,\n","                 mixing_probs=None,\n","                 epsilon=1e-6,\n","                 callback = None,\n","                 features = None):\n","        \"\"\"\n","        Arguments:\n","        n_components -- number of mixtures (components) to fit\n","        means -- (optional) initial array of mean vectors (numpy array of numpy arrays)\n","        covariances -- (optional) initial array of covariance matrices (numpy array of numpy arrays)\n","        mixing_probs -- (optional) initial vector (numpy array) of mixing probabilities\n","        epsilon -- (optional) convergence criterion\n","        \"\"\"\n","        self.n_components = n_components\n","        self.means = means\n","        self.covariances = covariances\n","        self.mixing_probs = mixing_probs\n","        self.epsilon = epsilon\n","        self.__callback = callback\n","        self.data = features\n","\n","    def fit(self, features):\n","        \"\"\"\n","        Fits a GMM into a set of feature data.\n","        Arguments:\n","        features -- input features data set\n","        \"\"\"\n","        # Initialise\n","        self.data = features\n","        n, _ = features.shape\n","        norm_densities = np.empty((n, self.n_components), np.float)\n","        responsibilities = np.empty((n, self.n_components), np.float)\n","        old_log_likelihood = 0\n","        self._initialise_parameters(features)\n","        print('features shape: ', features[0].shape)\n","        while True:\n","            # Compute normal densities\n","            for i in np.arange(n):\n","                x = features[i]\n","              \n","                for j in np.arange(self.n_components):\n","                    norm_densities[i][j] = self.multivariate_normal_pdf(x, self.means[j], self.covariances[j])\n","\n","            # Estimate log likelihood\n","            log_vector = np.log(np.array([np.dot(self.mixing_probs.T, norm_densities[i]) for i in np.arange(n)]))\n","            log_likelihood = np.dot(log_vector.T, np.ones(n))\n","            \n","            self.call_back()\n","\n","            # Check for convergence\n","            if np.absolute(log_likelihood - old_log_likelihood) < self.epsilon:\n","                break\n","\n","            # E-step: evaluate responsibilities\n","            for i in np.arange(n):\n","                x = features[i]\n","                denominator = np.dot(self.mixing_probs.T, norm_densities[i])\n","                for j in np.arange(self.n_components):\n","                    responsibilities[i][j] = self.mixing_probs[j] * norm_densities[i][j] / denominator\n","\n","            # M-step: re-estimate the parameters\n","            for i in np.arange(self.n_components):\n","                responsibility = (responsibilities.T)[i]\n","\n","                # Common denominator\n","                denominator = np.dot(responsibility.T, np.ones(n))\n","\n","                # Update mean\n","                self.means[i] = np.dot(responsibility.T, features) / denominator\n","                #print('means[i] : ', self.means[i].shape)\n","                # Update covariance\n","                difference = features - np.tile(self.means[i], (n, 1))\n","                self.covariances[i] = np.dot(np.multiply(responsibility.reshape(n,1), difference).T, difference) / denominator\n","                #print('covariances[i] : ', self.covariances[i].shape)\n","                # Update mixing probabilities\n","                self.mixing_probs[i] = denominator / n\n","                #print('mixing_probs[i] : ', self.mixing_probs[i].shape)\n","\n","            old_log_likelihood = log_likelihood\n","\n","    def cluster(self, features):\n","        \"\"\"\n","        Returns a numpy array containing partitioned feature data. The\n","        distance measure used to compute the distance between a feature point\n","        and a Gaussian distribution is Mahanalobis distance.\n","        \"\"\"\n","        # Initialise\n","        n, _ = features.shape\n","        partition = np.empty(n, np.int)\n","        distances = np.empty(self.n_components, np.float)\n","        cov_inverses = [np.linalg.inv(cov) for cov in self.covariances]\n","\n","        # Assign each feature point to a Gaussian distribution\n","        for i in np.arange(n):\n","          x = features[i]\n","\n","            # Compute Mahanalobis distances from each mixture\n","          for j in np.arange(self.n_components):\n","              distances[j] = np.dot(np.dot((x - self.means[j]).T, cov_inverses[j]), x - self.means[j])\n","\n","            # Find index of the minimum distance, and assign to a cluster\n","          partition[i] = np.argmin(distances)\n","\n","        return partition\n","\n","    def call_back(self):\n","        if self.__callback:\n","            dct = {\n","                'mixing_probs': self.mixing_probs,\n","                'means': self.means,\n","                'covariances': self.covariances\n","            }\n","            self.__callback(dct)\n","\n","    def multivariate_normal_pdf(self, x, mean, covariance):\n","        \"\"\"\n","        Returns normal density value for an n-dimensional random\n","        vector x.\n","        \"\"\"\n","        centered = x - mean\n","        cov_inverse = np.linalg.inv(covariance)\n","        cov_det = np.linalg.det(covariance)\n","        exponent = np.dot(np.dot(centered.T, cov_inverse), centered)\n","        return np.exp(-0.5 * exponent) / np.sqrt(cov_det * np.power(2 * np.pi, self.n_components))\n","\n","    def _initialise_parameters(self, features):\n","        \"\"\"\n","        Initialises parameters: means, covariances, and mixing probabilities\n","        if undefined.\n","        Arguments:\n","        features -- input features data set\n","        \"\"\"\n","        if not self.means or not self.covariances:\n","            n, m = features.shape\n","\n","            print('features before shuffled: ', features.shape)\n","            # Shuffle features set\n","            indices = np.arange(n)\n","            np.random.shuffle(np.arange(n))\n","            features_shuffled = np.array([features[i] for i in indices])\n","            print('features_shuffled: ', features_shuffled.shape)\n","            print()\n","\n","            # Split into n_components subarrays\n","            divs = int(np.floor(n / self.n_components))\n","            features_split = [features_shuffled[i:i+divs] for i in range(0, n, divs)]\n","            print('features_split : ', len(features_split[0]))\n","            # Estimate means/covariances (or both)\n","            if not self.means:\n","                means = []\n","                for i in np.arange(self.n_components):\n","                    means.append(np.mean(features_split[i], axis=0))\n","                    print('features split [', i, '] shape : ', features_split[i].shape)\n","                self.means = np.array(means)\n","                print('mean shape: ', self.means.shape)\n","\n","            if not self.covariances:\n","                covariances = []\n","                for i in np.arange(self.n_components):\n","                    covariances.append(np.cov(features_split[i].T))\n","                    print('cov   features split [', i, '] shape : ', features_split[i].shape)\n","                self.covariances = np.array(covariances)\n","                print('cov shape: ', self.covariances.shape)\n","\n","        if not self.mixing_probs:\n","            self.mixing_probs = np.repeat(1 / self.n_components, self.n_components)\n","            print('mixing_probs shape: ', self.mixing_probs.shape)\n","\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H-gs8TBiFWNF","colab_type":"code","outputId":"ebc4e410-ea86-4ffc-a953-998625339395","colab":{"base_uri":"https://localhost:8080/","height":258}},"source":["#MY DATASET\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from scipy.stats import norm\n","\n","def remove_outlier(df_in, col_name):\n","    q1 = df_in[col_name].quantile(0.25)\n","    q3 = df_in[col_name].quantile(0.75)\n","    iqr = q3-q1 #Interquartile range\n","    fence_low  = q1-1.5*iqr\n","    fence_high = q3+1.5*iqr\n","    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n","    #return df_out\n","\n","def impute_mean(series):\n","    return series.fillna(series.mean())\n","\n","dataset = pd.read_csv('data.csv',encoding=\"ISO-8859-1\") # dataset incorporation\n","dataset.drop(['stn_code','agency','sampling_date','location_monitoring_station'], axis=1,inplace=True)\n","\n","remove_outlier(dataset,'so2')\n","remove_outlier(dataset,'no2')\n","remove_outlier(dataset,'rspm')\n","remove_outlier(dataset,'spm')\n","\n","by_State=dataset.groupby('state')\n","dataset['rspm'] = by_State['rspm'].transform(impute_mean)\n","dataset['so2'] = by_State['so2'].transform(impute_mean)\n","dataset['no2'] = by_State['no2'].transform(impute_mean)\n","dataset['spm'] = by_State['spm'].transform(impute_mean)\n","dataset['pm2_5'] = by_State['pm2_5'].transform(impute_mean)\n","\n","# Missing values being filled in columns\n","for col in dataset.columns.values:\n","    if dataset[col].isnull().sum() == 0:\n","        continue\n","    if col == 'date':\n","        guess_values = dataset.groupby('state')['date'].apply(lambda x: x.mode().max())\n","    elif col=='type':\n","        guess_values = dataset.groupby('state')['type'].apply(lambda x: x.mode().max())\n","    else:\n","        guess_values = dataset.groupby('state')['location'].apply(lambda x: x.mode().max())\n","dataset.head()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n","  interactivity=interactivity, compiler=compiler, result=result)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>state</th>\n","      <th>location</th>\n","      <th>type</th>\n","      <th>so2</th>\n","      <th>no2</th>\n","      <th>rspm</th>\n","      <th>spm</th>\n","      <th>pm2_5</th>\n","      <th>date</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Andhra Pradesh</td>\n","      <td>Hyderabad</td>\n","      <td>Residential, Rural and other Areas</td>\n","      <td>4.8</td>\n","      <td>17.4</td>\n","      <td>78.182824</td>\n","      <td>200.260378</td>\n","      <td>NaN</td>\n","      <td>1990-02-01</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Andhra Pradesh</td>\n","      <td>Hyderabad</td>\n","      <td>Industrial Area</td>\n","      <td>3.1</td>\n","      <td>7.0</td>\n","      <td>78.182824</td>\n","      <td>200.260378</td>\n","      <td>NaN</td>\n","      <td>1990-02-01</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Andhra Pradesh</td>\n","      <td>Hyderabad</td>\n","      <td>Residential, Rural and other Areas</td>\n","      <td>6.2</td>\n","      <td>28.5</td>\n","      <td>78.182824</td>\n","      <td>200.260378</td>\n","      <td>NaN</td>\n","      <td>1990-02-01</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Andhra Pradesh</td>\n","      <td>Hyderabad</td>\n","      <td>Residential, Rural and other Areas</td>\n","      <td>6.3</td>\n","      <td>14.7</td>\n","      <td>78.182824</td>\n","      <td>200.260378</td>\n","      <td>NaN</td>\n","      <td>1990-03-01</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Andhra Pradesh</td>\n","      <td>Hyderabad</td>\n","      <td>Industrial Area</td>\n","      <td>4.7</td>\n","      <td>7.5</td>\n","      <td>78.182824</td>\n","      <td>200.260378</td>\n","      <td>NaN</td>\n","      <td>1990-03-01</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            state   location  ... pm2_5        date\n","0  Andhra Pradesh  Hyderabad  ...   NaN  1990-02-01\n","1  Andhra Pradesh  Hyderabad  ...   NaN  1990-02-01\n","2  Andhra Pradesh  Hyderabad  ...   NaN  1990-02-01\n","3  Andhra Pradesh  Hyderabad  ...   NaN  1990-03-01\n","4  Andhra Pradesh  Hyderabad  ...   NaN  1990-03-01\n","\n","[5 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"Npl1UKIky-BJ","colab_type":"code","outputId":"49f1351a-ecba-4810-e132-8a2b829fd03e","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# Derivation for Individual Pollutant Index and AQI STARTS HERE\n","\n","# EPA METHOD FORMULA\n","#  AQI_{P} = AQI_{min} +  ({PM_{Obs}-PM_{Min}}/{(PM_{Max}-PM_{Min})})* {AQI_{Max}-AQI_{Min}}\n","\n","# calculating AQI index of SO2 pollutant by EPA method formula given above\n","#  SO2 is scaled between 0-1600\n","def calculate_si(so2):\n","    si=0\n","    if (so2<=40):\n","     si = so2 * (50/40)\n","    elif (so2>40 and so2<=80):\n","     si = 50 + (so2-40) * (50/(80-40))\n","    elif (so2>80 and so2<=380):\n","     si = 100 + (so2-80) * (100/(380-80))\n","    elif (so2>380 and so2<=800):\n","     si = 200 + (so2-380) * (100/(800-380))\n","    elif (so2>800 and so2<=1600):\n","     si = 300 + (so2-800) * (100/(1600-800))\n","    elif (so2>1600):\n","     si = 400 + (so2-1600) * (100/800)\n","    return si\n","\n","# calling the function to calulate so2 pollutant index\n","dataset['si'] = dataset['so2'].apply(calculate_si)\n","df_si = dataset[['so2','si']]\n","df_si.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>so2</th>\n","      <th>si</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4.8</td>\n","      <td>6.000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3.1</td>\n","      <td>3.875</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>6.2</td>\n","      <td>7.750</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6.3</td>\n","      <td>7.875</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4.7</td>\n","      <td>5.875</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   so2     si\n","0  4.8  6.000\n","1  3.1  3.875\n","2  6.2  7.750\n","3  6.3  7.875\n","4  4.7  5.875"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"oNVJ8t5wzDrm","colab_type":"code","outputId":"8fe48843-0375-44bf-d370-fb76700a75b9","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["#Function to calculate no2 individual pollutant index(ni)\n","\n","# EPA METHOD FORMULA\n","#  AQI_{PM} = AQI_{min} +  ({PM_{Obs}-PM_{Min}}/{(PM_{Max}-PM_{Min})})* {AQI_{Max}-AQI_{Min}}\n","\n","# calculating AQI index of NO2 pollutant by EPA method formula given above\n","#  NO2 is scaled between 0-400\n","def calculate_ni(no2):\n","    ni = 0\n","    if(no2<=40):\n","     ni = no2*50/40\n","    elif(no2>40 and no2<=80):\n","     ni = 50 + (no2-40)*(50/(80-40))\n","    elif(no2>80 and no2<=180):\n","     ni = 100 + (no2-80)*(100/(180-80))\n","    elif(no2>180 and no2<=280):\n","     ni = 200 + (no2-180)*(100/(280-180))\n","    elif(no2>280 and no2<=400):\n","     ni = 300 + (no2-280)*(100/(400-280))\n","    else:\n","     ni = 400 + (no2-400)*(100/120)\n","    return ni\n","\n","# calling the function to calulate so2 pollutant index\n","dataset['ni'] = dataset['no2'].apply(calculate_ni)\n","df_ni = dataset[['no2','ni']]\n","df_ni.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>no2</th>\n","      <th>ni</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>17.4</td>\n","      <td>21.750</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7.0</td>\n","      <td>8.750</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>28.5</td>\n","      <td>35.625</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>14.7</td>\n","      <td>18.375</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7.5</td>\n","      <td>9.375</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    no2      ni\n","0  17.4  21.750\n","1   7.0   8.750\n","2  28.5  35.625\n","3  14.7  18.375\n","4   7.5   9.375"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"Wid0CUe5zG08","colab_type":"code","outputId":"22e34f0d-d7ca-44ee-d1b0-8ab2e83fcb41","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["#Function to calculate rspm individual pollutant index(rpi)\n","\n","# EPA METHOD FORMULA\n","#  AQI_{PM} = AQI_{min} +  ({PM_{Obs}-PM_{Min}}/{(PM_{Max}-PM_{Min})})* {AQI_{Max}-AQI_{Min}}\n","\n","# calculating AQI index of RSPM pollutant by EPA method formula given above\n","#  RSPM is scaled between 0-400\n","def calculate_(rspm):\n","    rpi=0\n","    if(rpi<=30):\n","     rpi = rpi*50/30\n","    elif(rpi>30 and rpi<=60):\n","     rpi = 50+(rpi-30)*50/(60-30)\n","    elif(rpi>60 and rpi<=90):\n","     rpi = 100+(rpi-60)*100/(90-60)\n","    elif(rpi>90 and rpi<=120):\n","     rpi = 200+(rpi-90)*100/(120-90)\n","    elif(rpi>120 and rpi<=250):\n","     rpi = 300+(rpi-120)*(100/(250-120))\n","    else:\n","     rpi = 400+(rpi-250)*(100/130)\n","    return rpi\n","\n","# calling the function to calulate RSPM pollutant index\n","dataset['rpi']=dataset['rspm'].apply(calculate_si)\n","df_rpi = dataset[['rspm','rpi']]\n","df_rpi.head()\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>rspm</th>\n","      <th>rpi</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>78.182824</td>\n","      <td>97.72853</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>78.182824</td>\n","      <td>97.72853</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>78.182824</td>\n","      <td>97.72853</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>78.182824</td>\n","      <td>97.72853</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>78.182824</td>\n","      <td>97.72853</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        rspm       rpi\n","0  78.182824  97.72853\n","1  78.182824  97.72853\n","2  78.182824  97.72853\n","3  78.182824  97.72853\n","4  78.182824  97.72853"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"ec297Fk9zJWf","colab_type":"code","outputId":"23675c29-a771-420a-e034-f6bdb45fe812","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["#Function to calculate spm individual pollutant index(spi)\n","\n","# EPA METHOD FORMULA\n","#  AQI_{PM} = AQI_{min} +  ({PM_{Obs}-PM_{Min}}/{(PM_{Max}-PM_{Min})})* {AQI_{Max}-AQI_{Min}}\n","\n","# calculating AQI index of SPM pollutant by EPA method formula given above\n","#  SPM is scaled between 0-400\n","def calculate_spi(spm):\n","    spi=0\n","    if(spm<=50):\n","     spi = spm*50/50\n","    elif(spm>50 and spm<=100):\n","     spi = 50 + (spm-50)*(50/(100-50))\n","    elif(spm>100 and spm<=250):\n","     spi = 100 + (spm-100)*(100/(250-100))\n","    elif(spm>250 and spm<=350):\n","     spi=200 + (spm-250)*(100/(350-250))\n","    elif(spm>350 and spm<=430):\n","     spi=300 + (spm-350)*(100/(430-350))\n","    else:\n","     spi=400+(spm-430)*(100/430)\n","    return spi\n","\n","# calling the function to calulate SPM pollutant index\n","dataset['spi'] = dataset['spm'].apply(calculate_spi)\n","df_spm = dataset[['spm','spi']]\n","df_spm.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>spm</th>\n","      <th>spi</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>200.260378</td>\n","      <td>166.840252</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>200.260378</td>\n","      <td>166.840252</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>200.260378</td>\n","      <td>166.840252</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>200.260378</td>\n","      <td>166.840252</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>200.260378</td>\n","      <td>166.840252</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          spm         spi\n","0  200.260378  166.840252\n","1  200.260378  166.840252\n","2  200.260378  166.840252\n","3  200.260378  166.840252\n","4  200.260378  166.840252"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"m--fTdA-zLEp","colab_type":"code","outputId":"a35168cd-58fe-4994-fd03-daaac7a7d9f2","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["#function to calculate the air quality index (AQI) of every data value its is calculated as per indian govt standards\n","\n","# AQI = MAX ( AQI_{SO2}, AQI_{NO2}, AQI_{RSPM}, AQI_{SPM})\n","def calculate_aqi(si,ni,spi,rpi):\n","    aqi=0\n","    if(si>ni and si>spi and si>rpi):\n","     aqi=si\n","    if(spi>si and spi>ni and spi>rpi):\n","     aqi=spi\n","    if(ni>si and ni>spi and ni>rpi):\n","     aqi=ni\n","    if(rpi>si and rpi>ni and rpi>spi):\n","     aqi=rpi\n","    return aqi\n","\n","# calling the function to calulate AQI \n","dataset['AQI'] = dataset.apply(lambda x:calculate_aqi(x['si'],x['ni'],x['spi'],x['rpi']),axis=1)\n","df= dataset[['state','si','ni','rpi','spi','AQI']]\n","df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>state</th>\n","      <th>si</th>\n","      <th>ni</th>\n","      <th>rpi</th>\n","      <th>spi</th>\n","      <th>AQI</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Andhra Pradesh</td>\n","      <td>6.000</td>\n","      <td>21.750</td>\n","      <td>97.72853</td>\n","      <td>166.840252</td>\n","      <td>166.840252</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Andhra Pradesh</td>\n","      <td>3.875</td>\n","      <td>8.750</td>\n","      <td>97.72853</td>\n","      <td>166.840252</td>\n","      <td>166.840252</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Andhra Pradesh</td>\n","      <td>7.750</td>\n","      <td>35.625</td>\n","      <td>97.72853</td>\n","      <td>166.840252</td>\n","      <td>166.840252</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Andhra Pradesh</td>\n","      <td>7.875</td>\n","      <td>18.375</td>\n","      <td>97.72853</td>\n","      <td>166.840252</td>\n","      <td>166.840252</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Andhra Pradesh</td>\n","      <td>5.875</td>\n","      <td>9.375</td>\n","      <td>97.72853</td>\n","      <td>166.840252</td>\n","      <td>166.840252</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            state     si      ni       rpi         spi         AQI\n","0  Andhra Pradesh  6.000  21.750  97.72853  166.840252  166.840252\n","1  Andhra Pradesh  3.875   8.750  97.72853  166.840252  166.840252\n","2  Andhra Pradesh  7.750  35.625  97.72853  166.840252  166.840252\n","3  Andhra Pradesh  7.875  18.375  97.72853  166.840252  166.840252\n","4  Andhra Pradesh  5.875   9.375  97.72853  166.840252  166.840252"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"J3Dq8RCjAz2T","colab_type":"code","colab":{}},"source":["dataset.fillna(0.0, inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ka9gPQuAW84G","colab_type":"code","colab":{}},"source":["data = dataset[['so2', 'no2']]\n","data = data[:500]\n","data = np.array(data)\n","\n","# Initialize GMM algorithm\n","n_components = 2\n","gmm_model = GMM(n_components)\n","\n","# Fit GMM to the data\n","gmm_model.fit(data)\n","\n","# Cluster data\n","labelled = gmm_model.cluster(data)\n","\n","# Plot clustered data with the location of Gaussian mixtures\n","plt.figure()\n","\n","# Plot contours of Gaussian mixtures\n","for mean, cov in zip(gmm_model.means, gmm_model.covariances):\n","    # Create grid\n","    mean_x = mean[0]\n","    std_x = np.sqrt(cov[0][0])\n","    mean_y = mean[1]\n","    std_y = np.sqrt(cov[1][1])\n","    x = np.linspace(mean_x - 3*std_x, mean_x + 3*std_x, 100)\n","    y = np.linspace(mean_y - 3*std_y, mean_y + 3*std_y, 100)\n","    X, Y = np.meshgrid(x, y)\n","\n","    # Tabulate pdf values\n","    Z = np.empty(X.shape, np.float)\n","\n","    for i in np.arange(X.shape[0]):\n","        for j in np.arange(X.shape[1]):\n","            v = np.array([X[i][j], Y[i][j]])\n","            Z[i][j] = gmm_model.multivariate_normal_pdf(v, mean, cov)\n","\n","    # Plot contours\n","    print('its contour')\n","    plt.contour(X, Y, Z)\n","\n","# Plot features assigned to each Gaussian mixture\n","markers = ['o', '+']\n","colors = ['r', 'b']\n","print('final plot')\n","print(data.shape)\n","print(labelled)\n","for d, l  in zip(data, labelled):\n","    plt.scatter(d[0], d[1], color=colors[l], marker=markers[l])\n","plt.savefig('scatter_plot.pdf')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wG5KUJPEdFoP","colab_type":"code","colab":{}},"source":["data = dataset[['no2', 'spm']]\n","data = data[:500]\n","data = np.array(data)\n","\n","\n","# Initialize GMM algorithm\n","n_components = 2\n","gmm_model = GMM(n_components)\n","\n","# Fit GMM to the data\n","gmm_model.fit(data)\n","\n","# Cluster data\n","labelled = gmm_model.cluster(data)\n","\n","# Plot clustered data with the location of Gaussian mixtures\n","plt.figure()\n","\n","# Plot contours of Gaussian mixtures\n","for mean, cov in zip(gmm_model.means, gmm_model.covariances):\n","    # Create grid\n","    mean_x = mean[0]\n","    std_x = np.sqrt(cov[0][0])\n","    mean_y = mean[1]\n","    std_y = np.sqrt(cov[1][1])\n","    x = np.linspace(mean_x - 3*std_x, mean_x + 3*std_x, 100)\n","    y = np.linspace(mean_y - 3*std_y, mean_y + 3*std_y, 100)\n","    X, Y = np.meshgrid(x, y)\n","\n","    # Tabulate pdf values\n","    Z = np.empty(X.shape, np.float)\n","\n","    for i in np.arange(X.shape[0]):\n","        for j in np.arange(X.shape[1]):\n","            v = np.array([X[i][j], Y[i][j]])\n","            Z[i][j] = gmm_model.multivariate_normal_pdf(v, mean, cov)\n","\n","    # Plot contours\n","    print('its contour')\n","    plt.contour(X, Y, Z)\n","\n","# Plot features assigned to each Gaussian mixture\n","markers = ['o', '+']\n","colors = ['r', 'g']\n","print('final plot')\n","print(data.shape)\n","print(labelled)\n","for d, l  in zip(data, labelled):\n","    plt.scatter(d[0], d[1], color=colors[l], marker=markers[l])\n","plt.savefig('scatter_plot.pdf')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GC6M6QgXzEPo","colab_type":"code","colab":{}},"source":["data = dataset[['so2', 'spm']]\n","data = data[:500]\n","data = np.array(data)\n","\n","\n","# Initialize GMM algorithm\n","n_components = 2\n","gmm_model = GMM(n_components)\n","\n","# Fit GMM to the data\n","gmm_model.fit(data)\n","\n","# Cluster data\n","labelled = gmm_model.cluster(data)\n","\n","# Plot clustered data with the location of Gaussian mixtures\n","plt.figure()\n","\n","# Plot contours of Gaussian mixtures\n","for mean, cov in zip(gmm_model.means, gmm_model.covariances):\n","    # Create grid\n","    mean_x = mean[0]\n","    std_x = np.sqrt(cov[0][0])\n","    mean_y = mean[1]\n","    std_y = np.sqrt(cov[1][1])\n","    x = np.linspace(mean_x - 3*std_x, mean_x + 3*std_x, 100)\n","    y = np.linspace(mean_y - 3*std_y, mean_y + 3*std_y, 100)\n","    X, Y = np.meshgrid(x, y)\n","\n","    # Tabulate pdf values\n","    Z = np.empty(X.shape, np.float)\n","\n","    for i in np.arange(X.shape[0]):\n","        for j in np.arange(X.shape[1]):\n","            v = np.array([X[i][j], Y[i][j]])\n","            Z[i][j] = gmm_model.multivariate_normal_pdf(v, mean, cov)\n","\n","    # Plot contours\n","    print('its contour')\n","    plt.contour(X, Y, Z)\n","\n","# Plot features assigned to each Gaussian mixture\n","markers = ['o', '+']\n","colors = ['r', 'g']\n","print('final plot')\n","print(data.shape)\n","print(labelled)\n","for d, l  in zip(data, labelled):\n","    plt.scatter(d[0], d[1], color=colors[l], marker=markers[l])\n","plt.savefig('scatter_plot.pdf')"],"execution_count":0,"outputs":[]}]}